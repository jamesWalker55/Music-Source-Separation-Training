{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, NamedTuple\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import demix_track, demix_track_demucs, get_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def measure_time(text: str):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"{text}: {elapsed_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:0\")\n",
    "    else:\n",
    "        print(\"CUDA is not avilable. Run inference on CPU. It will be very slow...\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bs_roformer model: results/model_bs_roformer_ep_317_sdr_12.9755.ckpt\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\Music-Source-Separation-Training\\.venv\\Lib\\site-packages\\torch\\functional.py:665: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window can are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at ..\\aten\\src\\ATen\\native\\SpectralOps.cpp:842.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bs_roformer model: results/model_bs_roformer_ep_937_sdr_10.5309.ckpt\n",
      "Loading htdemucs model: results/f7e0c4bc-ba3fe64a.th\n",
      "Loading htdemucs model: results/d12395a8-e57c48e6.th\n",
      "Load models: 3.12 sec\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Model:\n",
    "    type: str\n",
    "    config_path: str\n",
    "    checkpoint_path: str\n",
    "\n",
    "    # I don't know how to type these\n",
    "    # model: ???\n",
    "    # config: ???\n",
    "\n",
    "    def load_model(self):\n",
    "        print(f\"Loading {self.type} model: {self.checkpoint_path}\")\n",
    "        model, config = get_model_from_config(self.type, self.config_path)\n",
    "\n",
    "        state_dict = torch.load(self.checkpoint_path)\n",
    "        if self.type == \"htdemucs\":\n",
    "            # Fix for htdemucs pround etrained models\n",
    "            if \"state\" in state_dict:\n",
    "                state_dict = state_dict[\"state\"]\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "    def demix(self, mix: np.ndarray) -> dict[str, np.ndarray]:\n",
    "        mix = torch.tensor(mix.T, dtype=torch.float32)\n",
    "        if self.type == \"htdemucs\":\n",
    "            res = demix_track_demucs(self.config, self.model, mix, device)\n",
    "        else:\n",
    "            res = demix_track(self.config, self.model, mix, device)\n",
    "\n",
    "        for k in res:\n",
    "            res[k] = res[k].T\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "# Vocal model: BS Roformer (viperx edition)\n",
    "VOCAL_MODEL = Model(\n",
    "    \"bs_roformer\",\n",
    "    \"configs/viperx/model_bs_roformer_ep_317_sdr_12.9755.yaml\",\n",
    "    \"results/model_bs_roformer_ep_317_sdr_12.9755.ckpt\",\n",
    ")\n",
    "\n",
    "# Single stem model: BS Roformer (viperx edition)\n",
    "OTHER_MODEL = Model(\n",
    "    \"bs_roformer\",\n",
    "    \"configs/viperx/model_bs_roformer_ep_937_sdr_10.5309.yaml\",\n",
    "    \"results/model_bs_roformer_ep_937_sdr_10.5309.ckpt\",\n",
    ")\n",
    "\n",
    "# Single stem model: HTDemucs4 FT Drums\n",
    "DRUMS_MODEL = Model(\n",
    "    \"htdemucs\", \"configs/config_musdb18_htdemucs.yaml\", \"results/f7e0c4bc-ba3fe64a.th\"\n",
    ")\n",
    "\n",
    "# Single stem model: HTDemucs4 FT Bass\n",
    "BASS_MODEL = Model(\n",
    "    \"htdemucs\", \"configs/config_musdb18_htdemucs.yaml\", \"results/d12395a8-e57c48e6.th\"\n",
    ")\n",
    "\n",
    "with measure_time(\"Load models\"):\n",
    "    VOCAL_MODEL.load_model()\n",
    "    OTHER_MODEL.load_model()\n",
    "    DRUMS_MODEL.load_model()\n",
    "    BASS_MODEL.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAL_MODEL.config.training.instruments = ['vocals', 'other']\n",
      "OTHER_MODEL.config.training.instruments = ['vocals', 'other']\n",
      "DRUMS_MODEL.config.training.instruments = ['drums', 'bass', 'other', 'vocals']\n",
      "BASS_MODEL.config.training.instruments = ['drums', 'bass', 'other', 'vocals']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{VOCAL_MODEL.config.training.instruments = }\")\n",
    "print(f\"{OTHER_MODEL.config.training.instruments = }\")\n",
    "print(f\"{DRUMS_MODEL.config.training.instruments = }\")\n",
    "print(f\"{BASS_MODEL.config.training.instruments = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path: str):\n",
    "    # mix, sr = sf.read(path)\n",
    "    mix, sr = librosa.load(path, sr=44100, mono=False)\n",
    "    mix = mix.T\n",
    "\n",
    "    # Convert mono to stereo if needed\n",
    "    if len(mix.shape) == 1:\n",
    "        mix = np.stack([mix, mix], axis=-1)\n",
    "\n",
    "    return mix, sr\n",
    "\n",
    "\n",
    "def preview_audio(mix, sr):\n",
    "    import IPython.display\n",
    "\n",
    "    return IPython.display.Audio(data=mix.T, rate=sr, normalize=False)\n",
    "\n",
    "\n",
    "path = R\"D:\\Soundtracks\\Electronic\\Cametek (Camellia)\\[KCCD-007] [2019.08.12] Confetto x かめりあ - ごーいん!\\05. インターネットが遅いさん (Super-Slow-Internet-san).mp3\"\n",
    "\n",
    "\n",
    "try:\n",
    "    mix, sr = load_audio(path)\n",
    "except Exception as e:\n",
    "    print(f\"Can't read track: {path}\")\n",
    "    print(f\"Error message: {e}\")\n",
    "    raise\n",
    "\n",
    "preview_audio(mix, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programs\\Music-Source-Separation-Training\\.venv\\Lib\\site-packages\\torch\\backends\\cuda\\__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "c:\\Programs\\Music-Source-Separation-Training\\models\\bs_roformer\\attend.py:84: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  out = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocals = VOCAL_MODEL.demix(mix)['vocals']\n",
    "vocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = mix - vocals\n",
    "preview_audio(inst, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = OTHER_MODEL.demix(inst)['other']\n",
    "preview_audio(other, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_and_bass = inst - other\n",
    "preview_audio(drum_and_bass, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['drums', 'bass', 'other', 'vocals'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = DRUMS_MODEL.demix(mix)\n",
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(res['drums'], sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bass = BASS_MODEL.demix(drum_and_bass)[\"bass\"]\n",
    "preview_audio(bass, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(drum_and_bass - bass, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drums = DRUMS_MODEL.demix(drum_and_bass - bass)[\"drums\"]\n",
    "preview_audio(drums, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = drum_and_bass - bass - drums\n",
    "preview_audio(residual, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(residual + bass + drums + other + vocals - mix, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "x = Path(path)\n",
    "x.with_stem(x.stem + '_vocals').with_suffix('.flac')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
